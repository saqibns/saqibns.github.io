<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-01-14T15:09:49+05:30</updated><id>http://localhost:4000/</id><title type="html">Saqib’s Blog</title><subtitle>Trying to get better at things, little by little</subtitle><author><name>Saqib Nizam Shamsi</name></author><entry><title type="html">A List of Machine Learning Challenges in 2018</title><link href="http://localhost:4000/2018/01/14/list-of-ML-competitions-2018.html" rel="alternate" type="text/html" title="A List of Machine Learning Challenges in 2018" /><published>2018-01-14T00:00:00+05:30</published><updated>2018-01-14T12:25:36+05:30</updated><id>http://localhost:4000/2018/01/14/list-of-ML-competitions-2018</id><content type="html" xml:base="http://localhost:4000/2018/01/14/list-of-ML-competitions-2018.html">&lt;p&gt;Competitions are a great way to excel in machine learning. They offer various advantages in addtion to gaining knowledge and developing your skillset.&lt;/p&gt;

&lt;p&gt;The problems and goals are very welll defined. This saves you from the hassle of coming up with a problem, defining the goals rigorously, which are both achievable and non-trivial. You are also provided with data, which in most cases is ready for use. Someone has already done the painstaking work of collecting, preprocessing and organizing data. If it’s a competition on supervised learning, you also get labels for the data.&lt;/p&gt;

&lt;p&gt;If you’re a procrastinator, you have deadlines to your rescue. They keep you focused and prevent you from going astray ;)&lt;/p&gt;

&lt;p&gt;Competition leaderboards (if the competition has one), push you to do better. They keep things in perspective by giving continuous feedback on how you’re doing relative to others. You struggle to find better solutions, try to surpass yourself, and in the process keep growing.&lt;/p&gt;

&lt;p&gt;Finally, the rewards. They come in various forms. Monetary rewards are one. The satisfaction of solving a challenging problems and growing is another. But the main motivation for writing this post is the third kind of reward. If you’re a top performer in a competition organized under a conference, you get a chance to publish your results.&lt;/p&gt;

&lt;p&gt;I was looking for a curated list of such competitions but couldn’t find any. So, decided to make one. The table below summarizes all the competitons I could find. They have been ordered according to their deadlines. I plan on updating the list on a regular basis. As more conferences release information about the competitions on their website, I’ll add them to the list.&lt;/p&gt;

&lt;p&gt;If you know of any competition that is not on the list, please let me know in the comments or feel free to send a pull request.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;Conference&lt;/th&gt;
      &lt;th&gt;Starts&lt;/th&gt;
      &lt;th&gt;Ends&lt;/th&gt;
      &lt;th&gt;Website&lt;/th&gt;
      &lt;th&gt;Sub-&lt;br /&gt;Challenges&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a name=&quot;ntire-cvpr-table&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#ntire-cvpr&quot;&gt;New Trends in Image Restoration and Enhancement (NTIRE) Challenge&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://cvpr2018.thecvf.com/program/workshops&quot;&gt;CVPR&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;10th January&lt;/td&gt;
      &lt;td&gt;27th February&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://www.vision.ee.ethz.ch/en/ntire18/&quot;&gt;Link&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a name=&quot;ug2-table&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#ug2&quot;&gt;UG&lt;sup&gt;2&lt;/sup&gt; Prize Challenge&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://cvpr2018.thecvf.com/program/workshops&quot;&gt;CVPR&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;15th January&lt;/td&gt;
      &lt;td&gt;2nd April&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://www.ug2challenge.org/&quot;&gt;Link&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a name=&quot;clic-table&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#clic&quot;&gt;Challenge on Learned Image Compression (CLIC)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://cvpr2018.thecvf.com/program/workshops&quot;&gt;CVPR&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;24th December, ‘17&lt;/td&gt;
      &lt;td&gt;22nd April&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://www.compression.cc/challenge/&quot;&gt;Link&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a name=&quot;landmark-table&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#landmark&quot;&gt;Large-Scale Landmark Recognition&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://cvpr2018.thecvf.com/program/workshops&quot;&gt;CVPR&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;1st January&lt;/td&gt;
      &lt;td&gt;1st May&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://landmarkscvprw18.github.io/&quot;&gt;Link&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a name=&quot;robust-vision-table&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#robust-vision&quot;&gt;Robust Vision Challenge&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://cvpr2018.thecvf.com/program/workshops&quot;&gt;CVPR&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;1st February&lt;/td&gt;
      &lt;td&gt;15th May&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://www.robustvision.net/&quot;&gt;Link&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a name=&quot;activitynet-table&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#activitynet&quot;&gt;ActivityNet Large-Scale Activity Recognition Challenge&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://cvpr2018.thecvf.com/program/workshops&quot;&gt;CVPR&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;7th December, ‘17&lt;/td&gt;
      &lt;td&gt;TBA&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://activity-net.org/challenges/2018/index.html&quot;&gt;Link&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a name=&quot;kddcup-table&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#kddcup&quot;&gt;KDD Cup&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://www.kdd.org/kdd2018/&quot;&gt;KDD&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;1st March (Expected)&lt;/td&gt;
      &lt;td&gt;TBA&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://www.kdd.org/News/view/kdd-cup-2018-call-for-proposals&quot;&gt;Link&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a name=&quot;djirobomaster-table&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#djirobomaster&quot;&gt;DJI RoboMaster AI Challenge&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://icra2018.org/&quot;&gt;ICRA&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;TBA&lt;/td&gt;
      &lt;td&gt;TBA&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://icra2018.org/dji-robomaster-ai-challenge/&quot;&gt;Link&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a name=&quot;microrobotics-table&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#microrobotics&quot;&gt;Mobile Microrobotics Challenge&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://icra2018.org/&quot;&gt;ICRA&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;TBA&lt;/td&gt;
      &lt;td&gt;TBA&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://icra2018.org/mobile-microrobotics-challenge-2018/&quot;&gt;Link&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a name=&quot;interspeech-table&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#interspeech&quot;&gt;Interspeech Computational Paralinguistics ChallengE (ComParE)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://interspeech2018.org/&quot;&gt;Interspeech&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;TBA&lt;/td&gt;
      &lt;td&gt;TBA&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://compare.openaudio.eu/&quot;&gt;Link&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a name=&quot;aicity-table&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#aicity&quot;&gt;Nvidia AI City Challenge&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://cvpr2018.thecvf.com/program/workshops&quot;&gt;CVPR&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;TBA&lt;/td&gt;
      &lt;td&gt;TBA&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://www.aicitychallenge.org/&quot;&gt;Link&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a name=&quot;lowpowerir-table&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#lowpowerir&quot;&gt;Low-Power Image Recognition Challenge&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://cvpr2018.thecvf.com/program/workshops&quot;&gt;CVPR&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;TBA&lt;/td&gt;
      &lt;td&gt;TBA&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rebootingcomputing.ieee.org/lpirc&quot;&gt;Link&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a name=&quot;lip-table&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#lip&quot;&gt;The Look Into Person (LIP) Challenge&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://cvpr2018.thecvf.com/program/workshops&quot;&gt;CVPR&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;TBA&lt;/td&gt;
      &lt;td&gt;TBA&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://vuhcs.github.io/&quot;&gt;Link&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a name=&quot;davis-table&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#davis&quot;&gt;DAVIS Challenge on Video Object Segmentation&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://cvpr2018.thecvf.com/program/workshops&quot;&gt;CVPR&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;TBA&lt;/td&gt;
      &lt;td&gt;TBA&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://davischallenge.org/challenge2018/&quot;&gt;Link&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a name=&quot;tidyup-table&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#tidyup&quot;&gt;Tidy Up My Room Challenge&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://icra2018.org/&quot;&gt;ICRA&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;TBA&lt;/td&gt;
      &lt;td&gt;TBA&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://icra2018.org/tidy-up-my-room-challenge/&quot;&gt;Link&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;new-trends-in-image-restoration-and-enhancement-ntire-challenge&quot;&gt;&lt;a name=&quot;ntire-cvpr&quot;&gt;&lt;/a&gt;New Trends in Image Restoration and Enhancement (NTIRE) Challenge&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;NTIRE 2018 challenge on image super-resolution&lt;/strong&gt;
In order to gauge the current state-of-the-art in (example-based) single-image super-resolution under realistic conditions, to compare and to promote different solutions we are organizing an NTIRE challenge in conjunction with the CVPR 2018 conference.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;The challenge has 3 tracks:&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Track 1:&lt;/strong&gt; classic bicubic  uses the bicubic downscaling (Matlab imresize), the most common setting from the recent single-image super-resolution literature.
&lt;strong&gt;Track 2:&lt;/strong&gt; realistic mild adverse conditions  assumes that the degradation operators (emulating the image acquisition process from a digital camera) can be estimated through training pairs of low and high-resolution images. The degradation operators are the same within an image space and for all the images.
&lt;strong&gt;Track 3:&lt;/strong&gt; realistic difficult adverse conditions  assumes that the degradation operators (emulating the image acquisition process from a digital camera) can be estimated through training pairs of low and high-resolution images. The degradation operators are the same within an image space and for all the images.
&lt;strong&gt;Track 4:&lt;/strong&gt; realistic wild conditions assumes that the degradation operators (emulating the image acquisition process from a digital camera) can be estimated through training pairs of low and high images. The degradation operators are the same within an image space but DIFFERENT from one image to another. This setting is the closest to real “wild” conditions.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;NTIRE 2018 challenge on image dehazing&lt;/strong&gt; 
In order to gauge the current state-of-the-art in image dehazing for real haze as well as synthesized haze, to compare and to promote different solutions we are organizing an NTIRE challenge in conjunction with the CVPR 2018 conference. A novel dataset of real and synthesized hazy images with ground truth will be introduced with the challenge. It is the first image dehazing online challenge.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;The challenge has 3 tracks:&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Track 1:&lt;/strong&gt; realistic haze uses synthesized hazy images, a common setting from the recent image dehazing literature.
&lt;strong&gt;Track 2:&lt;/strong&gt; real haze with ground truth
&lt;strong&gt;Track 3:&lt;/strong&gt; real haze with color reference&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;NTIRE 2018 challenge on spectral reconstruction from RGB images&lt;/strong&gt;
In order to gauge the current state-of-the-art in spectral reconstruction from RGB images, to compare and to promote different solutions we are organizing an NTIRE challenge in conjunction with the CVPR 2018 conference. The largest dataset to date will be introduced with the challenge. It is the first spectral reconstruction from RGB images online challenge.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;The challenge has 2 tracks:&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Track 1:&lt;/strong&gt; “Clean”  recovering hyperspectral data from uncompressed 8-bit RGB images created by applying a know response function to ground truth hyperspectral information.
&lt;strong&gt;Track 2:&lt;/strong&gt; “Real World”  recovering hyperspectral data from jpg-compressed 8-bit RGB images created by applying an unknown response function to ground truth hyperspectral information.&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;http://www.vision.ee.ethz.ch/en/ntire18/&quot;&gt;Challenge Website&lt;/a&gt; | &lt;a href=&quot;#ntire-cvpr-table&quot;&gt;Back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;ug2-prize-challenge&quot;&gt;&lt;a name=&quot;ug2&quot;&gt;&lt;/a&gt;UG&lt;sup&gt;2&lt;/sup&gt; Prize Challenge&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;What is the current state-of-the art for image restoration and enhancement applied to images acquired under less than ideal circumstances?&lt;/p&gt;

  &lt;p&gt;Can the application of enhancement algorithms as a pre-processing step improve image interpretability for manual analysis or automatic visual recognition to classify scene content?&lt;/p&gt;

  &lt;p&gt;The UG&lt;sup&gt;2&lt;/sup&gt; Challenge seeks to answer these important questions for general applications related to computational photography and scene understanding. As a well-defined case study, the challenge aims to advance the analysis of images collected by small UAVs by improving image restoration and enhancement algorithm performance using the UG&lt;sup&gt;2&amp;lt;/sup Dataset.&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;http://www.ug2challenge.org/&quot;&gt;Challenge Website&lt;/a&gt; | &lt;a href=&quot;#ug2-table&quot;&gt;Back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;challenge-on-learned-image-compression-clic&quot;&gt;&lt;a name=&quot;clic&quot;&gt;&lt;/a&gt;Challenge on Learned Image Compression (CLIC)&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;Recent advances in machine learning have led to an increased interest in applying neural networks to the problem of compression.
We propose hosting an image-compression challenge which specifically targets methods which have been traditionally overlooked, with a focus on neural networks (but also welcomes traditional approaches). Such methods typically consist of an encoder subsystem, taking images and producing representations which are more easily compressed than the pixel representation (e.g., it could be a stack of convolutions, producing an integer feature map), which is then followed by an arithmetic coder. The arithmetic coder uses a probabilistic model of integer codes in order to generate a compressed bit stream. The compressed bit stream makes up the file to be stored or transmitted. In order to decompress this bit stream, two additional steps are needed: first, an arithmetic decoder, which has a shared probability model with the encoder. This reconstructs (losslessly) the integers produced by the encoder. The last step consists of another decoder producing a reconstruction of the original image.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;http://www.compression.cc/challenge/&quot;&gt;Challenge Website&lt;/a&gt; | &lt;a href=&quot;#clic-table&quot;&gt;Back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;large-scale-landmark-recognition&quot;&gt;&lt;a name=&quot;landmark&quot;&gt;&lt;/a&gt;Large-Scale Landmark Recognition&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;This workshop is to foster research on image retrieval and landmark recognition by introducing a novel large-scale dataset, together with evaluation protocols. More details will be available soon.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://landmarkscvprw18.github.io/&quot;&gt;Challenge Website&lt;/a&gt; | &lt;a href=&quot;#landmark-table&quot;&gt;Back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;robust-vision-challenge&quot;&gt;&lt;a name=&quot;robust-vision&quot;&gt;&lt;/a&gt;Robust Vision Challenge&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;The increasing availability of large annotated datasets such as Middlebury, PASCAL VOC, ImageNet, MS COCO, KITTI and Cityscapes has lead to tremendous progress in computer vision and machine learning over the last decade. Public leaderboards make it easy to track the state-of-the-art in the field by comparing the results of dozens of methods side-by-side. While steady progress is made on each individual dataset, many of them are limited to specific domains. KITTI, for example, focuses on real-world urban driving scenarios, while Middlebury considers indoor scenes and VIPER provides synthetic imagery in various weather conditions. Consequently, methods that are state-of-the-art on one dataset often perform worse on a different one or require substantial adaptation of the model parameters.&lt;/p&gt;

  &lt;p&gt;The goal of this workshop is to foster the development of vision systems that are robust and consequently perform well on a variety of datasets with different characteristics. Towards this goal, we propose the Robust Vision Challenge, where performance on several tasks (eg, reconstruction, optical flow, semantic/instance segmentation, single image depth prediction) is measured across a number of challenging benchmarks with different characteristics, e.g., indoors vs. outdoors, real vs. synthetic, sunny vs. bad weather, different sensors. We encourage submissions of novel algorithms, techniques which are currently in review and methods that have already been published.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;http://www.robustvision.net/&quot;&gt;Challenge Website&lt;/a&gt; | &lt;a href=&quot;#robust-vision-table&quot;&gt;Back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;activitynet-large-scale-activity-recognition-challenge&quot;&gt;&lt;a name=&quot;activitynet&quot;&gt;&lt;/a&gt;ActivityNet Large-Scale Activity Recognition Challenge&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;This challenge is the 3rd annual installment of the ActivityNet Large-Scale Activity Recognition Challenge, which was first hosted during CVPR 2016. It focuses on the recognition of daily life, high-level, goal-oriented activities from user-generated videos as those found in internet video portals.&lt;/p&gt;

  &lt;p&gt;We are proud to announce that this year the challenge will hosts seven diverse tasks which aim to push the limits of semantic visual understanding of videos as well as bridging visual content with human captions. Three out of the seven tasks in the challenge are based on the &lt;a href=&quot;http://activity-net.org/&quot;&gt;ActivityNet dataset&lt;/a&gt;, which was introduced in CVPR 2015 and organized hierarchically in a semantic taxonomy. These tasks focus on trace evidence of activities in time in the form of actionness/proposals, class labels, and &lt;a href=&quot;http://cs.stanford.edu/people/ranjaykrishna/densevid/&quot;&gt;captions&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;http://activity-net.org/challenges/2018/index.html&quot;&gt;Challenge Website&lt;/a&gt; | &lt;a href=&quot;#activitynet-table&quot;&gt;Back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;kdd-cup&quot;&gt;&lt;a name=&quot;kddcup&quot;&gt;&lt;/a&gt;KDD Cup&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;SIGKDD-2018 will take place in London, UK in August 2018. The KDD Cup competition is anticipated to last for 2-4 months, and the winners will be notified by mid-June. The winners will be honored at the KDD conference opening ceremony and will present their solutions at the KDD Cup workshop during the conference. The winners are expected to be monetarily rewarded, with the first prize being in the ballpark of ten thousand dollars.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;http://www.kdd.org/News/view/kdd-cup-2018-call-for-proposals&quot;&gt;Challenge Website&lt;/a&gt; | &lt;a href=&quot;#kddcup-table&quot;&gt;Back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;dji-robomaster-ai-challenge&quot;&gt;&lt;a name=&quot;djirobomaster&quot;&gt;&lt;/a&gt;DJI RoboMaster AI Challenge&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;DJI started RoboMaster in 2015 as an educational robotics competition for talented engineers and scientists. The annual RoboMaster competition requires teams to build robots that use shooting mechanisms to battle with other robots. The performances of the robots are monitored by a specially designed referee system, converting projectile hits into health point deductions on hit robots. To visit past games and introductory videos visit &lt;a href=&quot;https://www.twitch.tv/robomaster&quot;&gt;https://www.twitch.tv/robomaster&lt;/a&gt;. To see the RoboMaster2018 promotional video, go to: &lt;a href=&quot;https://youtu.be/uI2uoV58pzQ&quot;&gt;https://youtu.be/uI2uoV58pzQ&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;Each team will build 1 – 2 automatic AI robots. Robots will compete in a 5m x 8m arena, filled with various obstacles. Participants will design robots that autonomously shoot plastic projectiles. The objective is outcompeting advanced official DJI robots in a battle of the wits.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;http://icra2018.org/dji-robomaster-ai-challenge/&quot;&gt;Challenge Website&lt;/a&gt; | &lt;a href=&quot;#djirobomaster-table&quot;&gt;Back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;mobile-microrobotics-challenge&quot;&gt;&lt;a name=&quot;microrobotics&quot;&gt;&lt;/a&gt;Mobile Microrobotics Challenge&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;The IEEE Robotics &amp;amp; Automations Society (RAS) Micro/Nano Robotics &amp;amp; Automation Technical Committee (MNRA) invites applicants to participate in the 2018 Mobile Microrobotics Challenge (MMC), in which microrobots on the order of the diameter of a human hair face off in tests of autonomy, accuracy, and assembly.&lt;/p&gt;

  &lt;p&gt;Teams can participate in up to three events:&lt;/p&gt;

  &lt;ol&gt;
    &lt;li&gt;Autonomous Manipulation &amp;amp; Accuracy Challenge: Microrobots must autonomously manipulate micro-components around fixed obstacles to a desired position and orientation superimposed on the substrate.  The objective is to manipulate the objects as precisely as possible to their goal locations and orientations in the shortest amount of time.&lt;/li&gt;
    &lt;li&gt;Microassembly Challenge:  Microrobots must assemble multiple microscale components inside a narrow channel in a fixed amount of time. This task simulates anticipated applications of microassembly, including manipulation within a human blood vessel and the assembly of components in nanomanufacturing.&lt;/li&gt;
    &lt;li&gt;MMC Showcase &amp;amp; Poster Session: Each team has an opportunity to showcase and demonstrate any advanced capabilities and/or functionality of their microrobot system. Each participating team will get one vote to determine the Best in Show winner.&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;http://icra2018.org/mobile-microrobotics-challenge-2018/&quot;&gt;Challenge Website&lt;/a&gt; | &lt;a href=&quot;#microrobotics-table&quot;&gt;Back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;interspeech-computational-paralinguistics-challenge-compare&quot;&gt;&lt;a name=&quot;interspeech&quot;&gt;&lt;/a&gt;Interspeech Computational Paralinguistics ChallengE (ComParE)&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;The &lt;strong&gt;Interspeech Computational Paralinguistics ChallengE (ComParE)&lt;/strong&gt; series is an open Challenge in the field of Computational Paralinguistics dealing with states and traits of speakers as manifested in their speech signal’s properties. The Challenges takes annually place at INTERSPEECH since 2009. Every year, we introduce new tasks as there still exists a multiplicity of not yet covered, but highly relevant paralinguistic phenomena. The Challenge addresses the Audio, Speech, and Signal Processing, Natural Language Processing, Artificial Intelligence, Machine Learning, Affective &amp;amp; Behavioural Computing, Human-Computer/Robot-Interaction, mHealth, Psychology, and Medicine communities, and any other interested participants.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;http://compare.openaudio.eu/&quot;&gt;Challenge Website&lt;/a&gt; | &lt;a href=&quot;#interspeech-table&quot;&gt;Back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;nvidia-ai-city-challenge&quot;&gt;&lt;a name=&quot;aicity&quot;&gt;&lt;/a&gt;Nvidia AI City Challenge&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;There will be 1 billion cameras by 2020. Transportation is one of the largest segments that can benefit from actionable insights derived from data captured by these cameras. Between traffic, signaling systems, transportation systems, infrastructure, and transit, the opportunity for insights from these cameras to make transportation systems safer and smarter is immense. Unfortunately, there are several reasons why these potential benefits have not yet materialized for this vertical. Poor data quality, the lack of labels for the data, and the lack of high quality models that can convert the data into actionable insights are some of the biggest impediments to unlocking the value of the data. There is also need for platforms that allow for appropriate analysis from edge to cloud, which will accelerate the development and deployment of these models. The NVIDIA AI City Challenge Workshop at CVPR 2018 will specifically focus on ITS problems such as&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;Estimating traffic flow and volume&lt;/li&gt;
    &lt;li&gt;Leveraging unsupervised approaches to detect anomalies such as lane violation, illegal U-turns, wrong-direction driving. This is the only way to get the humans in the loop pay attention to meaningful visual information&lt;/li&gt;
    &lt;li&gt;Multi-camera tracking, and object re-identification in urban environments.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://www.aicitychallenge.org/&quot;&gt;Challenge Website&lt;/a&gt; | &lt;a href=&quot;#aicity-table&quot;&gt;Back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;low-power-image-recognition-challenge&quot;&gt;&lt;a name=&quot;lowpowerir&quot;&gt;&lt;/a&gt;Low-Power Image Recognition Challenge&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;Detect all relevant objects in as many images as possible of a common test set from the ImageNet object detection data set within 10 minutes.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://rebootingcomputing.ieee.org/lpirc&quot;&gt;Challenge Website (Old)&lt;/a&gt; | &lt;a href=&quot;#lowpowerir-table&quot;&gt;Back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-look-into-person-lip-challenge&quot;&gt;&lt;a name=&quot;lip&quot;&gt;&lt;/a&gt;The Look Into Person (LIP) Challenge&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;Developing solutions to comprehensive human visual understanding in the wild scenarios, regarded as one of the most fundamental problems in compute vision, could have a crucial impact in many industrial application domains, such as autonomous driving, virtual reality, video surveillance, human-computer interaction and human behavior analysis. For example, human parsing and pose estimation are often regarded as the very first step for higher-level activity/event recognition and detection. Nonetheless, a large gap seems to exist between what is needed by the real-life applications and what is achievable based on modern computer vision techniques. The goal of this workshop is to allow researchers from the fields of human visual understanding and other disciplines to present their progress, communication and co-develop novel ideas that potentially shape the future of this area and further advance the performance and applicability of correspondingly built systems in real-world conditions.&lt;/p&gt;

  &lt;p&gt;To stimulate the progress on this research topic and attract more talents to work on this topic, we will also provide a first standard human parsing and pose benchmark on a new large-scale Look Into Person (LIP) dataset. This dataset is both larger and more challenging than similar previous ones in the sense that the new dataset contains 50,000 images with elaborated pixel-wise annotations with comprehensive 19 semantic human part labels and 2D human poses with 16 dense key points. The images collected from the real-world scenarios contain humans appearing with challenging poses and views, heavily occlusions, various appearances and low-resolutions. Details on the annotated classes and examples of our annotations are available at this link &lt;a href=&quot;http://hcp.sysu.edu.cn/lip/&quot;&gt;http://hcp.sysu.edu.cn/lip/&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://vuhcs.github.io/&quot;&gt;Challenge Website&lt;/a&gt; | &lt;a href=&quot;#lip-table&quot;&gt;Back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;davis-challenge-on-video-object-segmentation&quot;&gt;&lt;a name=&quot;davis&quot;&gt;&lt;/a&gt;DAVIS Challenge on Video Object Segmentation&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;We present the 2017 DAVIS Challenge, a public competition specifically designed for the task of video object segmentation. Following the footsteps of other successful initiatives, such as ILSVRC and PASCAL VOC, which established the avenue of research in the fields of scene classification and semantic segmentation, the DAVIS Challenge comprises a dataset, an evaluation methodology, and a public competition with a dedicated workshop co-located with CVPR 2017. The DAVIS Challenge follows up on the recent publication of DAVIS (Densely-Annotated VIdeo Segmentation), which has fostered the development of several novel state-of-the-art video object segmentation techniques. In this paper we describe the scope of the benchmark, highlight the main characteristics of the dataset and define the evaluation metrics of the competition.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;http://davischallenge.org/challenge2018/&quot;&gt;Challenge Website&lt;/a&gt; | &lt;a href=&quot;#davis-table&quot;&gt;Back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;tidy-up-my-room-challenge&quot;&gt;&lt;a name=&quot;tidyup&quot;&gt;&lt;/a&gt;Tidy Up My Room Challenge&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;Robust interaction in domestic settings is still a hard problem for most robots. These settings tend to be unstructured, changing and aimed at humans not robots. This makes the grasping and picking of a wide range of objects in a person’s home a canonical problem for future robotic applications. With this challenge, we aim to foster a community around solving these tasks in a holistic fashion, requiring a tight integration of perception, reasoning and actuation.&lt;/p&gt;

  &lt;p&gt;Robotics is an integration discipline and significant efforts are put in by labs worldwide every year to build robotic systems, yet it is hard to compare and validate these approaches against each other. Challenges and competitions have provided an opportunity to benchmark robotic systems on specific tasks, such as pick and place, and driving. We envision this challenge to contain multiple tasks and to increase in complexity over the years.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;http://icra2018.org/tidy-up-my-room-challenge/&quot;&gt;Challenge Website&lt;/a&gt; | &lt;a href=&quot;#tidyup-table&quot;&gt;Back&lt;/a&gt;&lt;/p&gt;

&lt;div id=&quot;disqus_thread&quot;&gt;
    &lt;button class=&quot;disqus-load&quot; onclick=&quot;loadDisqusComments()&quot;&gt;
      Load Comments
    &lt;/button&gt;
  &lt;/div&gt;
&lt;script&gt;

  /**
  *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW
  *  TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
  *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT:s
  *  https://disqus.com/admin/universalcode/#configuration-variables
  */
  var disqus_config = function () {
    this.page.url = &quot;http://localhost:4000/2018/01/14/list-of-ML-competitions-2018.html&quot;;
    this.page.identifier = &quot;&quot; ||
                           &quot;http://localhost:4000/2018/01/14/list-of-ML-competitions-2018.html&quot;;
  }
  function loadDisqusComments() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = '//saqibns-github-io.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  }
  &lt;/script&gt;

&lt;noscript&gt;
    Please enable JavaScript to view the
    &lt;a href=&quot;https://disqus.com/?ref_noscript&quot;&gt;comments powered by Disqus&lt;/a&gt;.
  &lt;/noscript&gt;</content><author><name>Saqib Nizam Shamsi</name></author><category term="conferences" /><category term="competition" /><category term="list" /><summary type="html">Try your hands on Machine Learning challenges organized under the umbrella of various conferences.</summary></entry><entry><title type="html">Predicting Emotions of a Group</title><link href="http://localhost:4000/2017/12/30/predicting-emotions-of-a-group.html" rel="alternate" type="text/html" title="Predicting Emotions of a Group" /><published>2017-12-30T00:00:00+05:30</published><updated>2017-12-03T13:48:50+05:30</updated><id>http://localhost:4000/2017/12/30/predicting-emotions-of-a-group</id><content type="html" xml:base="http://localhost:4000/2017/12/30/predicting-emotions-of-a-group.html">&lt;p&gt;&lt;a href=&quot;https://sites.google.com/site/emotiwchallenge/&quot;&gt;Emotion Recognition In the Wild (EmotiW)&lt;/a&gt; is a competition organized under the umbrella of &lt;a href=&quot;https://icmi.acm.org/2017/&quot;&gt;International Conference on Multimodal Interaction (ICMI)&lt;/a&gt;. The competition is being organized since 2013. The  competition for the year 2017 consisted of two sub-challenges :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Group Level Emotion Recognition&lt;/li&gt;
  &lt;li&gt;Audio-Video Emotion Recognition&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I participated in the former with &lt;a href=&quot;https://bhanu-mnit.github.io/&quot;&gt;Bhanu Pratap Singh Rawat&lt;/a&gt; and Manya Wadhwa. We were given a dataset which contained photographs of groups of people. The aim of the sub-challenge was to come up with a model that could classify the emotion the group as &lt;code class=&quot;highlighter-rouge&quot;&gt;Positive&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;Negative&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;Neural&lt;/code&gt;. This post describes our approach for the challenge.&lt;/p&gt;

&lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt;

&lt;p&gt;The dataset was divided into three parts: train, validation and test. Each part consisted of image files for each of our target categories: &lt;code class=&quot;highlighter-rouge&quot;&gt;Positive&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;Negative&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;Neutral&lt;/code&gt;. The distribution of images in the dataset is:&lt;/p&gt;

&lt;table class=&quot;mbtablestyle&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Set&lt;/th&gt;
      &lt;th&gt;Positive&lt;/th&gt;
      &lt;th&gt;Neutral&lt;/th&gt;
      &lt;th&gt;Negative&lt;/th&gt;
      &lt;th&gt;Total&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Train&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;1272&lt;/td&gt;
      &lt;td&gt;1199&lt;/td&gt;
      &lt;td&gt;1159&lt;/td&gt;
      &lt;td&gt;3630&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Validation&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;773&lt;/td&gt;
      &lt;td&gt;728&lt;/td&gt;
      &lt;td&gt;564&lt;/td&gt;
      &lt;td&gt;2065&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Test&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;311&lt;/td&gt;
      &lt;td&gt;165&lt;/td&gt;
      &lt;td&gt;296&lt;/td&gt;
      &lt;td&gt;772&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;A sample of images from the dataset is given below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/emotiw-17/group_snapshot.png&quot; alt=&quot;Sample Images in the EmotiW Dataset&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;method&quot;&gt;Method&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Summary:&lt;/strong&gt; We used ConvNets for our task with emotion heatmaps as features. If you wish to know the specifics of what emotion heatmaps are, read on!&lt;/p&gt;

&lt;h3 id=&quot;emotion-of-individual-faces&quot;&gt;Emotion of Individual Faces&lt;/h3&gt;

&lt;p&gt;The first step of our approach was to extract faces from each file and determine the emotion of each face. We extracted faces from the files using &lt;a href=&quot;http://dlib.net/&quot;&gt;Dlib&lt;/a&gt; and determined the emotion for each of the extracted faces. This was done using the pre-trained &lt;a href=&quot;https://www.openu.ac.il/home/hassner/projects/cnn_emotions/&quot;&gt;models&lt;/a&gt; by Gil Levi and Tal Hassner developed for EmotiW 2015 (we shall refer to them as &lt;em&gt;LHModels&lt;/em&gt; throughout the post). It is a set of five models which assign scores for seven standard emotions: Anger, Disgust, Fear, Happy, Neutral, Sad and Surprise. A single score for each face across the seven categories is obtained by averaging the five scores obtained for each category.&lt;/p&gt;

&lt;h3 id=&quot;combination-of-values&quot;&gt;Combination of Values&lt;/h3&gt;

&lt;p&gt;Once we obtained face wise emotions, a logical next step was to use the scores obtained for individual faces and somehow combine them to enable a learning algorithm to accurately determine the emotion of a group.&lt;/p&gt;

&lt;p&gt;We averaged five vectors obtained for every face in the image. The emotion with the highest value was said to be the overall emotion of the group. We then trained a Random Forest Classifier (15 estimators) with the averaged seven-dimensional vector for each image.&lt;/p&gt;

&lt;p&gt;For both of the cases above, if an image was labeled as one of Anger, Disgust, Fear, Sadness or Surprised the final label assigned to it was Negative, Neutral was assigned Neutral and Happy was assigned Positive. The performance for each of the approaches is summarized in a table later.&lt;/p&gt;

&lt;p&gt;We then used the predictions of LHModels to generate heatmaps for our images. The first step was to convert the seven-dimensional vectors into three-dimensional vectors representing our three categories of interest, &lt;code class=&quot;highlighter-rouge&quot;&gt;Positive&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;Negative&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Neutral&lt;/code&gt;. We did this by taking the average of Anger, Disgust, Fear, Sadness and Surprised designating it the label &lt;code class=&quot;highlighter-rouge&quot;&gt;Negative&lt;/code&gt;, Happy was given the label &lt;code class=&quot;highlighter-rouge&quot;&gt;Positive&lt;/code&gt;, and Neutral was assigned &lt;code class=&quot;highlighter-rouge&quot;&gt;Neutral&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;After this step, we had a set of three values for every face (one corresponding to each emotion). We used those to create three heatmaps, which, actually, are Gaussian interpolations of those values in a two-dimensional space.  The code snippet below creates a two-dimensional Gaussian kernel. The height and width of the kernel are same as that of the image in which the face was present. When interpolating, we observed that the values fell quickly as we went away from the center of the heatmap, since the distance increases rapidly. We used a value of 0.1 to make the values decrease gradually over distance (in other words, the effective distance from one pixel to the next was reduced to 1/10&lt;sup&gt;th&lt;/sup&gt; of its original value).&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;DISTANCE_SMOOTHING&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;make_gaussian&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;height&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;center&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fwhm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; Make a square gaussian kernel.
    fwhm is full-width-half-maximum, which
    can be thought of as an effective radius.
    &quot;&quot;&quot;&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;height&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;height&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;center&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;center&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DISTANCE_SMOOTHING&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DISTANCE_SMOOTHING&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fwhm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The kernel is then multiplied by the value of the emotion for that face. Let us take the face below as an example:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/emotiw-17/image_0_0.jpg&quot; alt=&quot;image_0_0&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The heatmaps (2D Gaussian distributions) for &lt;code class=&quot;highlighter-rouge&quot;&gt;Negative&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;Positive&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Neutral&lt;/code&gt; emotion values for the above face are:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/emotiw-17/image_0_0.jpg_gaussian_neg-crop.png&quot; alt=&quot;image_0_0.jpg_gaussian_neg-crop&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/emotiw-17/image_0_0.jpg_gaussian_neu-crop.png&quot; alt=&quot;image_0_0.jpg_gaussian_neu-crop&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/emotiw-17/image_0_0.jpg_gaussian_pos-crop.png&quot; alt=&quot;image_0_0.jpg_gaussian_pos-crop&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We, then used the individual heatmaps as channels of an RGB image, with distribution for &lt;code class=&quot;highlighter-rouge&quot;&gt;Negative&lt;/code&gt; emotion being the red channel and those of &lt;code class=&quot;highlighter-rouge&quot;&gt;Neutral&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Positive&lt;/code&gt; forming the green and the blue channels respectively. For the face shown above, the final heatmap looks like the image below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/emotiw-17/image_0_0.jpg_combined.png&quot; alt=&quot;image_0_0.jpg_combined&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We carry out this process for every face in an image and finally add the RGB images tensor together, thus forming a single image.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/emotiw-17/image-process.jpg&quot; alt=&quot;image process&quot; /&gt;&lt;/p&gt;

&lt;p&gt;​The image above demonstrates the entire process of converting an image to a heatmap. The final heatmap was then resized and fed to Convolutional Neural Networks. Using this methodology, we achieved a classification accuracy of 56.47% on the validation set. The baseline was 52.79%.&lt;/p&gt;

&lt;p&gt;A table summarizing the performance of the approaches is given below.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Model&lt;/th&gt;
      &lt;th&gt;Training Accuracy&lt;/th&gt;
      &lt;th&gt;Validation Accuracy&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Baseline&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;52.79%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Averaging&lt;/td&gt;
      &lt;td&gt;44.37%&lt;/td&gt;
      &lt;td&gt;42.38%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Random Forest&lt;/td&gt;
      &lt;td&gt;99.08%&lt;/td&gt;
      &lt;td&gt;48.13%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ConvNet on Heatmaps&lt;/td&gt;
      &lt;td&gt;54.16%​&lt;/td&gt;
      &lt;td&gt;56.47%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Please refer to our &lt;a href=&quot;https://arxiv.org/abs/1710.01216&quot;&gt;paper&lt;/a&gt; for more details.&lt;/p&gt;

&lt;div id=&quot;disqus_thread&quot;&gt;
    &lt;button class=&quot;disqus-load&quot; onclick=&quot;loadDisqusComments()&quot;&gt;
      Load Comments
    &lt;/button&gt;
  &lt;/div&gt;
&lt;script&gt;

  /**
  *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW
  *  TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
  *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT:s
  *  https://disqus.com/admin/universalcode/#configuration-variables
  */
  var disqus_config = function () {
    this.page.url = &quot;http://localhost:4000/2017/12/30/predicting-emotions-of-a-group.html&quot;;
    this.page.identifier = &quot;&quot; ||
                           &quot;http://localhost:4000/2017/12/30/predicting-emotions-of-a-group.html&quot;;
  }
  function loadDisqusComments() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = '//saqibns-github-io.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  }
  &lt;/script&gt;

&lt;noscript&gt;
    Please enable JavaScript to view the
    &lt;a href=&quot;https://disqus.com/?ref_noscript&quot;&gt;comments powered by Disqus&lt;/a&gt;.
  &lt;/noscript&gt;</content><author><name>Saqib Nizam Shamsi</name></author><category term="vision" /><category term="emotion detection" /><category term="competition" /><summary type="html">Emotion Recognition In the Wild is a competition organized under the umbrella of ICMI. This post describes our approach for the 2017 challenge.</summary></entry></feed>