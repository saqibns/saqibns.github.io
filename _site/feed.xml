<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <generator uri="http://jekyllrb.com" version="3.5.2">Jekyll</generator>
  
  
  <link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2017-12-30T04:35:46+05:30</updated>
  <id>http://localhost:4000//</id>

  
    <title type="html">Saqib’s Blog</title>
  

  
    <subtitle>Saqib's Blog. Here I discuss about mostly about Machine Learning and  some others things that interest me. 
</subtitle>
  

  
    <author>
        <name>Saqib Nizam Shamsi</name>
      
      
    </author>
  

  
  
    <entry>
      
      <title type="html">Predicting Emotions Of A Group</title>
      
      <link href="http://localhost:4000/2017/12/30/predicting-emotions-of-a-group/" rel="alternate" type="text/html" title="Predicting Emotions Of A Group" />
      <published>2017-12-30T01:32:28+05:30</published>
      <updated>2017-12-30T01:32:28+05:30</updated>
      <id>http://localhost:4000/2017/12/30/predicting-emotions-of-a-group</id>
      <content type="html" xml:base="http://localhost:4000/2017/12/30/predicting-emotions-of-a-group/">&lt;p&gt;&lt;a href=&quot;https://sites.google.com/site/emotiwchallenge/&quot;&gt;Emotion Recognition In the Wild (EmotiW)&lt;/a&gt; is a competition organized under the umbrella of &lt;a href=&quot;https://icmi.acm.org/2017/&quot;&gt;International Conference on Multimodal Interaction (ICMI)&lt;/a&gt;. The competition is being organized since 2013. The  competition for the year 2017 consisted of two sub-challenges :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Group Level Emotion Recognition&lt;/li&gt;
  &lt;li&gt;Audio-Video Emotion Recognition&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I participated in the former with Bhanu Pratap Singh Rawat and Manya Wadhwa. We were given a dataset which contained photographs of groups of people. The aim of the sub-challenge was to come up with a model that could classify the emotion the group as &lt;code class=&quot;highlighter-rouge&quot;&gt;Positive&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;Negative&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;Neural&lt;/code&gt;. This post describes our approach for the challenge.&lt;/p&gt;

&lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt;

&lt;p&gt;The dataset was divided into three parts: train, validation and test. Each part consisted of image files for each of our target categories: &lt;code class=&quot;highlighter-rouge&quot;&gt;Positive&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;Negative&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;Neutral&lt;/code&gt;. The distribution of images in the dataset is:&lt;/p&gt;

&lt;table class=&quot;mbtablestyle&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Set&lt;/th&gt;
      &lt;th&gt;Positive&lt;/th&gt;
      &lt;th&gt;Neutral&lt;/th&gt;
      &lt;th&gt;Negative&lt;/th&gt;
      &lt;th&gt;Total&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Train&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;1272&lt;/td&gt;
      &lt;td&gt;1199&lt;/td&gt;
      &lt;td&gt;1159&lt;/td&gt;
      &lt;td&gt;3630&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Validation&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;773&lt;/td&gt;
      &lt;td&gt;728&lt;/td&gt;
      &lt;td&gt;564&lt;/td&gt;
      &lt;td&gt;2065&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Test&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;311&lt;/td&gt;
      &lt;td&gt;165&lt;/td&gt;
      &lt;td&gt;296&lt;/td&gt;
      &lt;td&gt;772&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;A sample of images from the dataset is given below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/emotiw-17/group_snapshot.png&quot; alt=&quot;Sample Images in the EmotiW Dataset&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;method&quot;&gt;Method&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Summary:&lt;/strong&gt; We used ConvNets for our task with emotion heatmaps as features. If you wish to know the specifics of what emotion heatmaps are, read on!&lt;/p&gt;

&lt;h3 id=&quot;emotion-of-individual-faces&quot;&gt;Emotion of Individual Faces&lt;/h3&gt;

&lt;p&gt;The first step of our approach was to extract faces from each file and determine the emotion of each face. We extracted faces from the files using &lt;a href=&quot;http://dlib.net/&quot;&gt;Dlib&lt;/a&gt; and determined the emotion for each of the extracted faces. This was done using the pre-trained &lt;a href=&quot;https://www.openu.ac.il/home/hassner/projects/cnn_emotions/&quot;&gt;models&lt;/a&gt; by Gil Levi and Tal Hassner developed for EmotiW 2015 (we shall refer to them as &lt;em&gt;LHModels&lt;/em&gt; throughout the post). It is a set of five models which assign scores for seven standard emotions: Anger, Disgust, Fear, Happy, Neutral, Sad and Surprise. A single score for each face across the seven categories is obtained by averaging the five scores obtained for each category.&lt;/p&gt;

&lt;h3 id=&quot;combination-of-values&quot;&gt;Combination of Values&lt;/h3&gt;

&lt;p&gt;Once we obtained face wise emotions, a logical next step was to use the scores obtained for individual faces and somehow combine them to enable a learning algorithm to accurately determine the emotion of a group.&lt;/p&gt;

&lt;p&gt;We averaged five vectors obtained for every face in the image. The emotion with the highest value was said to be the overall emotion of the group. We then trained a Random Forest Classifier (15 estimators) with the averaged seven-dimensional vector for each image.&lt;/p&gt;

&lt;p&gt;For both of the cases above, if an image was labeled as one of Anger, Disgust, Fear, Sadness or Surprised the final label assigned to it was Negative, Neutral was assigned Neutral and Happy was assigned Positive. The performance for each of the approaches is summarized in a table later.&lt;/p&gt;

&lt;p&gt;We then used the predictions of LHModels to generate heatmaps for our images. The first step was to convert the seven-dimensional vectors into three-dimensional vectors representing our three categories of interest, &lt;code class=&quot;highlighter-rouge&quot;&gt;Positive&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;Negative&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Neutral&lt;/code&gt;. We did this by taking the average of Anger, Disgust, Fear, Sadness and Surprised designating it the label &lt;code class=&quot;highlighter-rouge&quot;&gt;Negative&lt;/code&gt;, Happy was given the label &lt;code class=&quot;highlighter-rouge&quot;&gt;Positive&lt;/code&gt;, and Neutral was assigned &lt;code class=&quot;highlighter-rouge&quot;&gt;Neutral&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;After this step, we had a set of three values for every face (one corresponding to each emotion). We used those to create three heatmaps, which, actually, are Gaussian interpolations of those values in a two-dimensional space.  The code snippet below creates a two-dimensional Gaussian kernel. The height and width of the kernel are same as that of the image in which the face was present. When interpolating, we observed that the values fell quickly as we went away from the center of the heatmap, since the distance increases rapidly. We used a value of 0.1 to make the values decrease gradually over distance (in other words, the effective distance from one pixel to the next was reduced to 1/10&lt;sup&gt;th&lt;/sup&gt; of its original value).&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;DISTANCE_SMOOTHING&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;make_gaussian&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;height&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;center&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fwhm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; Make a square gaussian kernel.
    fwhm is full-width-half-maximum, which
    can be thought of as an effective radius.
    &quot;&quot;&quot;&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;height&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;height&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;center&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;center&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DISTANCE_SMOOTHING&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DISTANCE_SMOOTHING&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fwhm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The kernel is then multiplied by the value of the emotion for that face. Let us take the face below as an example:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/emotiw-17/image_0_0.jpg&quot; alt=&quot;image_0_0&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The heatmaps (2D Gaussian distributions) for &lt;code class=&quot;highlighter-rouge&quot;&gt;Negative&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;Positive&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Neutral&lt;/code&gt; emotion values for the above face are:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/emotiw-17/image_0_0.jpg_gaussian_neg-crop.png&quot; alt=&quot;image_0_0.jpg_gaussian_neg-crop&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/emotiw-17/image_0_0.jpg_gaussian_neu-crop.png&quot; alt=&quot;image_0_0.jpg_gaussian_neu-crop&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/emotiw-17/image_0_0.jpg_gaussian_pos-crop.png&quot; alt=&quot;image_0_0.jpg_gaussian_pos-crop&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We, then used the individual heatmaps as channels of an RGB image, with distribution for &lt;code class=&quot;highlighter-rouge&quot;&gt;Negative&lt;/code&gt; emotion being the red channel and those of &lt;code class=&quot;highlighter-rouge&quot;&gt;Neutral&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Positive&lt;/code&gt; forming the green and the blue channels respectively. For the face shown above, the final heatmap looks like the image below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/emotiw-17/image_0_0.jpg_combined.png&quot; alt=&quot;image_0_0.jpg_combined&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We carry out this process for every face in an image and finally add the RGB images tensor together, thus forming a single image.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/emotiw-17/image-process.jpg&quot; alt=&quot;image process&quot; /&gt;&lt;/p&gt;

&lt;p&gt;​The image above demonstrates the entire process of converting an image to a heatmap. The final heatmap was then resized and fed to Convolutional Neural Networks. Using this methodology, we achieved a classification accuracy of 56.47% on the validation set. The baseline was 52.79%.&lt;/p&gt;

&lt;p&gt;A table summarizing the performance of the approaches is given below.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Model&lt;/th&gt;
      &lt;th&gt;Training Accuracy&lt;/th&gt;
      &lt;th&gt;Validation Accuracy&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Baseline&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;52.79%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Averaging&lt;/td&gt;
      &lt;td&gt;44.37%&lt;/td&gt;
      &lt;td&gt;42.38%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Random Forest&lt;/td&gt;
      &lt;td&gt;99.08%&lt;/td&gt;
      &lt;td&gt;48.13%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ConvNet on Heatmaps&lt;/td&gt;
      &lt;td&gt;54.16%​&lt;/td&gt;
      &lt;td&gt;56.47%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Please refer to our &lt;a href=&quot;https://arxiv.org/abs/1710.01216&quot;&gt;paper&lt;/a&gt; for more details.&lt;/p&gt;

&lt;div id=&quot;disqus_thread&quot;&gt;&lt;/div&gt;
&lt;script&gt;
    /**
     *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
     *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
     */
 
    var disqus_config = function () {
        this.page.url = saqibns.github.io;  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = &quot;/2017/12/30/predicting-emotions-of-a-group&quot;; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
 
    (function() {  // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        
        s.src = 'https://saqibns-github-io.disqus.com/embed.js';
        
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
&lt;/script&gt;

&lt;noscript&gt;Please enable JavaScript to view the &lt;a href=&quot;https://disqus.com/?ref_noscript&quot; rel=&quot;nofollow&quot;&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Saqib Nizam Shamsi</name>
          
          
        </author>
      

      
        <category term="vision" />
      
        <category term="emotion-detection" />
      
        <category term="competition" />
      

      

      
        <summary type="html">Emotion Recognition In the Wild (EmotiW) is a competition organized under the umbrella of International Conference on Multimodal Interaction (ICMI). The competition is being organized since 2013. The competition for the year 2017 consisted of two sub-challenges :</summary>
      

      
      
    </entry>
  
  
</feed>
